{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e26d042",
   "metadata": {},
   "source": [
    "Scikit-learn contains tools for machine learning and statisitcal modelling such as: regression, classification, clustering, dimensionality reduction  \n",
    "Sckit is a powerful and modern machine learning Python library for fully and semi automated data analysis and information extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778196d5",
   "metadata": {},
   "source": [
    "# INTRODUCTION TO MACHINE LEARNING AND SCIKIT- LEARN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d078405",
   "metadata": {},
   "source": [
    "Machine learning is the process of finding hidden data patterns and relationships \n",
    "to enable information driven decisions to be made\n",
    "    Types of machine learning:\n",
    "    \n",
    "    Supervised learning\n",
    "    Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bf82a1",
   "metadata": {},
   "source": [
    "Machine learning terminologies\n",
    "\n",
    "    Observations/Records/Samples/Examples\n",
    "    Features/inputs/Attributes are present as column names in a spreadsheet\n",
    "    Responses/labels/outcome/Target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f6d16f",
   "metadata": {},
   "source": [
    "ML begins with a problem or a dataset\n",
    "\n",
    "ML Steps\n",
    "\n",
    ". Understand the problem/dataset\n",
    "\n",
    ". Extract features from the dataset\n",
    "\n",
    ". Identify the problem type\n",
    "\n",
    ". Choose the right model\n",
    "\n",
    ". Train and test the model\n",
    "\n",
    ". Strive for accuracy- fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2d47e6",
   "metadata": {},
   "source": [
    "# Step 1 & 2: Understand the problem/dataset, Extract features from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b331891a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education.num</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>native.country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90</td>\n",
       "      <td>?</td>\n",
       "      <td>77053</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>?</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>Private</td>\n",
       "      <td>132870</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>18</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66</td>\n",
       "      <td>?</td>\n",
       "      <td>186061</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>?</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>Private</td>\n",
       "      <td>140359</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>4</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>Private</td>\n",
       "      <td>264663</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>310152</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>257302</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>154374</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>58</td>\n",
       "      <td>Private</td>\n",
       "      <td>151910</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>201490</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32561 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age workclass  fnlwgt     education  education.num      marital.status  \\\n",
       "0       90         ?   77053       HS-grad              9             Widowed   \n",
       "1       82   Private  132870       HS-grad              9             Widowed   \n",
       "2       66         ?  186061  Some-college             10             Widowed   \n",
       "3       54   Private  140359       7th-8th              4            Divorced   \n",
       "4       41   Private  264663  Some-college             10           Separated   \n",
       "...    ...       ...     ...           ...            ...                 ...   \n",
       "32556   22   Private  310152  Some-college             10       Never-married   \n",
       "32557   27   Private  257302    Assoc-acdm             12  Married-civ-spouse   \n",
       "32558   40   Private  154374       HS-grad              9  Married-civ-spouse   \n",
       "32559   58   Private  151910       HS-grad              9             Widowed   \n",
       "32560   22   Private  201490       HS-grad              9       Never-married   \n",
       "\n",
       "              occupation   relationship   race     sex  capital.gain  \\\n",
       "0                      ?  Not-in-family  White  Female             0   \n",
       "1        Exec-managerial  Not-in-family  White  Female             0   \n",
       "2                      ?      Unmarried  Black  Female             0   \n",
       "3      Machine-op-inspct      Unmarried  White  Female             0   \n",
       "4         Prof-specialty      Own-child  White  Female             0   \n",
       "...                  ...            ...    ...     ...           ...   \n",
       "32556    Protective-serv  Not-in-family  White    Male             0   \n",
       "32557       Tech-support           Wife  White  Female             0   \n",
       "32558  Machine-op-inspct        Husband  White    Male             0   \n",
       "32559       Adm-clerical      Unmarried  White  Female             0   \n",
       "32560       Adm-clerical      Own-child  White    Male             0   \n",
       "\n",
       "       capital.loss  hours.per.week native.country income  \n",
       "0              4356              40  United-States  <=50K  \n",
       "1              4356              18  United-States  <=50K  \n",
       "2              4356              40  United-States  <=50K  \n",
       "3              3900              40  United-States  <=50K  \n",
       "4              3900              40  United-States  <=50K  \n",
       "...             ...             ...            ...    ...  \n",
       "32556             0              40  United-States  <=50K  \n",
       "32557             0              38  United-States  <=50K  \n",
       "32558             0              40  United-States   >50K  \n",
       "32559             0              40  United-States  <=50K  \n",
       "32560             0              20  United-States  <=50K  \n",
       "\n",
       "[32561 rows x 15 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('adult.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6007007c",
   "metadata": {},
   "source": [
    "In the above table the features are age,workclass,education and more \n",
    "while target is the income because the main objective is to construct a model that \n",
    "can predict the income depending on the provided features while observations are values from row with index 0 to 32560"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c8c0ee",
   "metadata": {},
   "source": [
    "# Steps 3 & 4: Identify the problem type, Choose the right model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd3ac3e",
   "metadata": {},
   "source": [
    "Types of machine learning:\n",
    "    \n",
    "    Supervised learning\n",
    "    Unsupervised learning\n",
    "Supervised learning\n",
    "In this the dataset used to train the model should have observation features and target.\n",
    "The model is trained to predict the right response for a given set of data points.\n",
    "\n",
    "Supervised models are used to predict an outcome\n",
    "Its goal is to generalize a dataset so that a general rule can be applied to new data as well    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329648c8",
   "metadata": {},
   "source": [
    "In Unsupervised learning, the response or outcome of the data is unknown\n",
    "\n",
    "Unsupervised learning models are used to identify and visualize patterns in data by grouping similar types of data\n",
    "Its goal is to represent data in a way that meaningful information can be extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf60271",
   "metadata": {},
   "source": [
    "Problem types depend on the type of data ie: continuous or categorical\n",
    "\n",
    "\n",
    "In Supervised learning  If the data is categorical(grouped) then it requires a classification based solution eg. predicting whether someone has a disease or not\n",
    "\n",
    "If data is continuous(infinite values eg. age,temp) then it requires a regression based solution eg.  predicting someones age \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfabe31",
   "metadata": {},
   "source": [
    "In unsupervised learning  If the data is categorical then it requires a clustering based solution \n",
    "\n",
    "If data is continuous then it requires a dimensionality reduction based solution(reduces dimensions of data without affecting data) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ea8446",
   "metadata": {},
   "source": [
    "Supervised learning example: categories of news based on topics\n",
    "\n",
    "Unsupervised learning example: grouping similar stories on different news networks   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf6f7a0",
   "metadata": {},
   "source": [
    "## Working of Supervised and Unsupervised models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee561e5",
   "metadata": {},
   "source": [
    "In supervised learning, a known dataset with observations, \n",
    "features and response is used to create and train a machine learning algorithm\n",
    "\n",
    "Features and response are fed into the algorithm for training\n",
    "\n",
    "A predictive model built on top of this algorithm after fine tuning is then used to predict the response \n",
    "for a new dataset that has the same features \n",
    "\n",
    "Unseen data/New data/Testing data with no labels but similar features will be used for prediction in order to obtain the responses which will be used in comparison with the actual responses to obtain accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb67d1a",
   "metadata": {},
   "source": [
    "In unsupervised learning, a known dataset has a set of observations with features with no response/labels. \n",
    "\n",
    "Here the algorithm cant be taught to predict because there are no labels but based on your features and domain expertise you can choose a few assumptions that will help you define the features the algorithm should watch out for.Hence the predicitve model uses these features to identify how to classify and represent data points of new or unseen data. You can also use cross validation to further test and train the model to improve its accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7350c07",
   "metadata": {},
   "source": [
    "# Steps 5 & 6: Train and test the model, Strive for accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79a49c6",
   "metadata": {},
   "source": [
    "Only supervised learning models can be trained since the right features and labels are already known\n",
    "\n",
    "For training supervised learning models, data analysts divide a known dataset into training and testing sets\n",
    "Before this one has to identify attributes that affect the outcome\n",
    "Approaches to train supervised models:\n",
    "\n",
    ". Using 2 separate datasets: one for training the other for testing\n",
    "\n",
    ". Splitting a single dataset into training set and testing set where by the test set is about 20% -40% of original dataset       while training set is 60%-80% of original dataset. Here both the response and  features are spilt separately\n",
    "\n",
    "\n",
    "The split approach is highly preferred since it yields higher accuracy \n",
    "For unsupervised learning models, the algorithm looks for similarities based only on statistical properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf5aed7",
   "metadata": {},
   "source": [
    "# Supervised model considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29ced02",
   "metadata": {},
   "source": [
    "Responses and features which directly affect the model are considered\n",
    "\n",
    "Fine tune the model parameters based on training and testing results to optimize performance and accuracy \n",
    "\n",
    "Generalization means predicting the response. Strive for a model that can predict efficiently\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4d4695",
   "metadata": {},
   "source": [
    "# SCIKIT-LEARN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de8792a",
   "metadata": {},
   "source": [
    "It is a machine learning library for fully or semi automated data analysis and for information extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4f4acc",
   "metadata": {},
   "source": [
    "## Importance of Scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4d4395",
   "metadata": {},
   "source": [
    ". Efficient tools to identfy and organize problems(unsupervised/supervised)\n",
    "\n",
    ". Free and open datasets\n",
    "\n",
    ". Rich set of libraries for learning and predicting\n",
    "\n",
    ". Model support for every problem type\n",
    "\n",
    ". Model persistence ie. pickle\n",
    "\n",
    ". Open source community and vector support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8622b9f0",
   "metadata": {},
   "source": [
    "Scikit learn helps data scientists orgainze their work through its problem solution approach\n",
    "\n",
    "Problem solution approach involves:\n",
    "\n",
    "    . Model and algorithm selection based on dataset type \n",
    "    . Using the estimator object which represents the model by importing class and instantiating it\n",
    "    . Model training\n",
    "    . Predicting- Forecasting the responses of unseen data\n",
    "    . Model tuning through multiple iterations and result observations\n",
    "    . Striving for accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952be524",
   "metadata": {},
   "source": [
    "Problem solution considerations using Scikit learn:\n",
    "    \n",
    "    . Create separate objects for feature and response\n",
    "    . Ensure features and responses have only numeric values\n",
    "    . Features and response should be in the form of a numpy array\n",
    "    . Features are always mapped as x and responses are mapped as y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162da8b6",
   "metadata": {},
   "source": [
    "# Supervised learning models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65476bac",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42299a44",
   "metadata": {},
   "source": [
    "Used in analyzing continuous data\n",
    "\n",
    "It is easy to use since it doesn,t require a lot of tuning\n",
    "\n",
    "Runs very fast hence being time-efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fa4bba",
   "metadata": {},
   "source": [
    "Linear regression is based on the formula:\n",
    "    y=β1 x + β0 ,\n",
    "    \n",
    "basing the above equation on y=mx+c\n",
    "we can derive the following:\n",
    "\n",
    "    y=Response\n",
    "    β0=y-intercept\n",
    "    β1=coefficient of x\n",
    "    x=input features\n",
    "    β1= dy/dx\n",
    "The goal is to find estimated values of β1 and β0  which would provide the \"best\" fit(least square line/line of best fit) in some sense for the data points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6c801b",
   "metadata": {},
   "source": [
    "y= β1x + β0 + u\n",
    "\n",
    "where u is the residual value which is equal to difference\n",
    "\n",
    "between actual value and predicted value of y ie: u=y-[β1x + β0]\n",
    "\n",
    "residual is the distance between a datapoint and the line of best fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e3e459",
   "metadata": {},
   "source": [
    "SSR(Regression of sum of squares) is the sum of squared deviations of predicted values (predicted using regression) from the mean value, \n",
    "and SSE is the sum of squared deviations of actual values from predicted values.\n",
    "\n",
    "In statistics, the residual sum of squares (RSS), also known as the sum of squared residuals (SSR)\n",
    "or the sum of squared estimate of errors/Error of sum of squares (SSE), is the sum of the squares of residuals \n",
    "(deviations predicted from actual empirical values of data). \n",
    "\n",
    "It is a measure of the discrepancy between the data and an estimation model, such as a linear regression. A small RSS indicates a tight fit of the model to the data. It is used as an optimality criterion in parameter selection and model selection.\n",
    "\n",
    "Therefore the smaller the value of SSR or SSE the more accurate the prediction hence making the model to best fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b8f875",
   "metadata": {},
   "source": [
    "The attributes are usually fitted using the least square approach\n",
    "The method of least squares is a standard approach in regression analysis to approximate the solution of overdetermined systems \n",
    "by minimizing the sum of the squares of the residuals (a residual being the difference between an observed value \n",
    "and the fitted value provided by a model) made in the results of each individual equation.\n",
    "\n",
    "    SSR=Σ(y2-y1)^2\n",
    "\n",
    "    SSE=Σ(y-y2)^2\n",
    "\n",
    "    where:\n",
    "        y2 is the predicted value of outcome\n",
    "        y1 is the mean of the outcome ie: indicates that the slope of the line is 0\n",
    "        y is the actual value of outcome\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b137f6",
   "metadata": {},
   "source": [
    "sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False,copy_X=True,n_jobs=1)\n",
    "\n",
    "whereby:\n",
    "    \n",
    "    fit_intercept=True calculates the intercept for the model\n",
    "    normalize=False normalizes the regression variable before perfrming the regression operation\n",
    "    copy_X=True copies the regression variable else it will be overwritten and default value will be held true\n",
    "    n_jobs=1 represents the number of jobs to be computed in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f76d643",
   "metadata": {},
   "source": [
    "## LINEAR REGRESSION EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51d33ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32ae44ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes_dataset=load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aca4b782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _diabetes_dataset:\n",
      "\n",
      "Diabetes dataset\n",
      "----------------\n",
      "\n",
      "Ten baseline variables, age, sex, body mass index, average blood\n",
      "pressure, and six blood serum measurements were obtained for each of n =\n",
      "442 diabetes patients, as well as the response of interest, a\n",
      "quantitative measure of disease progression one year after baseline.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "  :Number of Instances: 442\n",
      "\n",
      "  :Number of Attributes: First 10 columns are numeric predictive values\n",
      "\n",
      "  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n",
      "\n",
      "  :Attribute Information:\n",
      "      - age     age in years\n",
      "      - sex\n",
      "      - bmi     body mass index\n",
      "      - bp      average blood pressure\n",
      "      - s1      tc, T-Cells (a type of white blood cells)\n",
      "      - s2      ldl, low-density lipoproteins\n",
      "      - s3      hdl, high-density lipoproteins\n",
      "      - s4      tch, thyroid stimulating hormone\n",
      "      - s5      ltg, lamotrigine\n",
      "      - s6      glu, blood sugar level\n",
      "\n",
      "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n",
      "\n",
      "Source URL:\n",
      "https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
      "\n",
      "For more information see:\n",
      "Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n",
      "(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n"
     ]
    }
   ],
   "source": [
    "# pd.DataFrame(boston_dataset)\n",
    "print(diabetes_dataset['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b53853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_columns=diabetes_dataset['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbe9a60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>-0.017646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068330</td>\n",
       "      <td>-0.092204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>-0.009362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031991</td>\n",
       "      <td>-0.046641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.019662</td>\n",
       "      <td>0.059744</td>\n",
       "      <td>-0.005697</td>\n",
       "      <td>-0.002566</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.031193</td>\n",
       "      <td>0.007207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>-0.005515</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>-0.067642</td>\n",
       "      <td>0.049341</td>\n",
       "      <td>0.079165</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>-0.018118</td>\n",
       "      <td>0.044485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>0.017282</td>\n",
       "      <td>-0.037344</td>\n",
       "      <td>-0.013840</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>-0.011080</td>\n",
       "      <td>-0.046879</td>\n",
       "      <td>0.015491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.016318</td>\n",
       "      <td>0.015283</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.026560</td>\n",
       "      <td>0.044528</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.073030</td>\n",
       "      <td>-0.081414</td>\n",
       "      <td>0.083740</td>\n",
       "      <td>0.027809</td>\n",
       "      <td>0.173816</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.004220</td>\n",
       "      <td>0.003064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>442 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0    0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1   -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2    0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
       "3   -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4    0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "437  0.041708  0.050680  0.019662  0.059744 -0.005697 -0.002566 -0.028674   \n",
       "438 -0.005515  0.050680 -0.015906 -0.067642  0.049341  0.079165 -0.028674   \n",
       "439  0.041708  0.050680 -0.015906  0.017282 -0.037344 -0.013840 -0.024993   \n",
       "440 -0.045472 -0.044642  0.039062  0.001215  0.016318  0.015283 -0.028674   \n",
       "441 -0.045472 -0.044642 -0.073030 -0.081414  0.083740  0.027809  0.173816   \n",
       "\n",
       "           s4        s5        s6  \n",
       "0   -0.002592  0.019908 -0.017646  \n",
       "1   -0.039493 -0.068330 -0.092204  \n",
       "2   -0.002592  0.002864 -0.025930  \n",
       "3    0.034309  0.022692 -0.009362  \n",
       "4   -0.002592 -0.031991 -0.046641  \n",
       "..        ...       ...       ...  \n",
       "437 -0.002592  0.031193  0.007207  \n",
       "438  0.034309 -0.018118  0.044485  \n",
       "439 -0.011080 -0.046879  0.015491  \n",
       "440  0.026560  0.044528 -0.025930  \n",
       "441 -0.039493 -0.004220  0.003064  \n",
       "\n",
       "[442 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes=pd.DataFrame(diabetes_dataset.data)\n",
    "diabetes.columns=diabetes_dataset.feature_names\n",
    "diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b629772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b3783db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_dataset.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1abee9ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
       "        69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
       "        68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
       "        87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
       "       259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
       "       128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
       "       150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
       "       200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
       "        42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
       "        83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
       "       104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
       "       173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
       "       107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
       "        60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
       "       197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
       "        59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
       "       237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
       "       143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
       "       142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
       "        77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
       "        78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
       "       154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
       "        71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
       "       150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
       "       145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
       "        94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
       "        60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
       "        31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
       "       114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
       "       191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
       "       244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
       "       263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
       "        77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
       "        58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
       "       140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
       "       219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
       "        43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
       "       140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
       "        84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
       "        94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
       "       220.,  57.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0869497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>diabeteslevel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>-0.017646</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068330</td>\n",
       "      <td>-0.092204</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>-0.009362</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031991</td>\n",
       "      <td>-0.046641</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.019662</td>\n",
       "      <td>0.059744</td>\n",
       "      <td>-0.005697</td>\n",
       "      <td>-0.002566</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.031193</td>\n",
       "      <td>0.007207</td>\n",
       "      <td>178.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>-0.005515</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>-0.067642</td>\n",
       "      <td>0.049341</td>\n",
       "      <td>0.079165</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>-0.018118</td>\n",
       "      <td>0.044485</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>0.017282</td>\n",
       "      <td>-0.037344</td>\n",
       "      <td>-0.013840</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>-0.011080</td>\n",
       "      <td>-0.046879</td>\n",
       "      <td>0.015491</td>\n",
       "      <td>132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.016318</td>\n",
       "      <td>0.015283</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.026560</td>\n",
       "      <td>0.044528</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.073030</td>\n",
       "      <td>-0.081414</td>\n",
       "      <td>0.083740</td>\n",
       "      <td>0.027809</td>\n",
       "      <td>0.173816</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.004220</td>\n",
       "      <td>0.003064</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>442 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0    0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1   -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2    0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
       "3   -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4    0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "437  0.041708  0.050680  0.019662  0.059744 -0.005697 -0.002566 -0.028674   \n",
       "438 -0.005515  0.050680 -0.015906 -0.067642  0.049341  0.079165 -0.028674   \n",
       "439  0.041708  0.050680 -0.015906  0.017282 -0.037344 -0.013840 -0.024993   \n",
       "440 -0.045472 -0.044642  0.039062  0.001215  0.016318  0.015283 -0.028674   \n",
       "441 -0.045472 -0.044642 -0.073030 -0.081414  0.083740  0.027809  0.173816   \n",
       "\n",
       "           s4        s5        s6  diabeteslevel  \n",
       "0   -0.002592  0.019908 -0.017646          151.0  \n",
       "1   -0.039493 -0.068330 -0.092204           75.0  \n",
       "2   -0.002592  0.002864 -0.025930          141.0  \n",
       "3    0.034309  0.022692 -0.009362          206.0  \n",
       "4   -0.002592 -0.031991 -0.046641          135.0  \n",
       "..        ...       ...       ...            ...  \n",
       "437 -0.002592  0.031193  0.007207          178.0  \n",
       "438  0.034309 -0.018118  0.044485          104.0  \n",
       "439 -0.011080 -0.046879  0.015491          132.0  \n",
       "440  0.026560  0.044528 -0.025930          220.0  \n",
       "441 -0.039493 -0.004220  0.003064           57.0  \n",
       "\n",
       "[442 rows x 11 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes['diabeteslevel']=diabetes_dataset['target']\n",
    "diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f22b9332",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_X= diabetes_dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7be94aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_Y=diabetes_dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b58636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import linear model which is the estimator\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linreg=LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0db6d645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg.fit(diabetes_X,diabetes_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b369466a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated intercept is 152.13\n"
     ]
    }
   ],
   "source": [
    "print('The estimated intercept is %.2f'%linreg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d433259c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated intercept is 10\n"
     ]
    }
   ],
   "source": [
    "#  coef_ contain the coefficients for the prediction of each of the targets. \n",
    "# It is also the same as if you trained a model to predict each of the targets separately.\n",
    "print('The estimated intercept is %d'%len(linreg.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3a15704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,Y_train,Y_test= train_test_split(diabetes_X,diabetes_Y, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0886d029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(353, 10) (89, 10) (353,) (89,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape,Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c0cae76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5a6c95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([184.48818324, 194.17192926, 167.22507354, 156.66162199,\n",
       "       232.31326575, 187.12920073, 160.09830154, 118.38082982,\n",
       "        83.17969415, 119.93321849, 183.02423235, 202.66853411,\n",
       "       208.75965546, 163.30865957, 159.60865041, 119.2617394 ,\n",
       "       125.14277585, 157.65575122,  47.24950869,  72.95155116,\n",
       "       114.31501786, 151.33803596, 119.77826795, 162.19180498,\n",
       "       101.21459627, 143.26665547,  82.96071343, 199.50519604,\n",
       "       102.67463175, 114.29749435, 216.83041971, 268.99311375,\n",
       "        56.98163081, 237.27943146, 174.42115623, 134.25275289,\n",
       "       140.80786277, 219.45129747, 220.79799732, 159.28616634,\n",
       "       273.09225627, 191.39514125,  58.05129685,  55.88985679,\n",
       "       127.18011844, 105.31488359, 197.9435066 , 210.0909035 ,\n",
       "       187.05953786,  99.25316552, 210.86043396, 216.70090938,\n",
       "       197.03899548, 168.16132473, 178.90283669, 157.14837293,\n",
       "        39.81627608, 121.31842128, 178.30181319, 217.91452754,\n",
       "       134.01129137, 185.93725688, 149.42894642, 122.68892531,\n",
       "       110.27394152, 171.7625744 ,  91.35239146,  78.62705832,\n",
       "       170.54378859, 232.1658917 , 175.66028733,  89.86558797,\n",
       "       177.87296238, 188.58008264,  99.78832681, 112.78381259,\n",
       "       253.04808969, 222.61036685,  39.58854291, 148.14829924,\n",
       "       239.02659283, 170.10559291,  75.56603843, 263.16010184,\n",
       "       146.98056117, 147.65797252, 101.43697369, 138.03848672,\n",
       "       144.85174402])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "569a588b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error is 3680.13\n"
     ]
    }
   ],
   "source": [
    "print('Mean squared error is %.2f' % np.mean((linreg.predict(X_test)-Y_test)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5aabdcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance is 0.41\n"
     ]
    }
   ],
   "source": [
    "print('Variance is %.2f' % linreg.score(X_test, Y_test))\n",
    "# The closer the value is to 1 the higher the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c429013",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf6251bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions=linreg.predict(X_test)\n",
    "for predic in predictions:\n",
    "    int (predic)\n",
    "    \n",
    "predictions=predictions.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f58790f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([262.,  67., 110., 131., 217., 101., 141., 109.,  93.,  79., 143.,\n",
       "       297., 198., 134., 120., 113., 150., 206.,  47., 200.,  90.,  95.,\n",
       "       183., 156.,  31., 116.,  65., 293.,  47.,  89., 248., 346.,  99.,\n",
       "       274., 252., 131.,  50., 332., 121., 144., 273., 142.,  57.,  78.,\n",
       "       253.,  65., 186., 279.,  91.,  69., 288., 155.,  68., 206.,  70.,\n",
       "        95., 116.,  67., 107., 275., 142.,  90., 168.,  60.,  88., 311.,\n",
       "       113.,  98., 109., 208.,  52.,  96., 180., 161.,  64.,  61., 263.,\n",
       "       192.,  55.,  97., 280., 121., 134., 220., 252.,  83.,  69., 168.,\n",
       "       172.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "009f7681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02247191011235955"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bc38f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cf82df1",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d760f0ad",
   "metadata": {},
   "source": [
    "It is a generailization of the linear regression model, used for classification problems. Used for categorical data\n",
    "\n",
    "It is used for predicting the categorical dependent variable using a given set of independent variables.\n",
    "\n",
    "Therefore the outcome must be a categorical or discrete value. It can be either Yes or No, 0 or 1, true or False, etc. but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d6fb2f",
   "metadata": {},
   "source": [
    "Logistic regression is basically a supervised classification algorithm\n",
    "In a classification problem, the target variable(or output), y, can take only discrete values for a given set of features(or inputs), X.\n",
    "Contrary to popular belief, logistic regression IS a regression model. \n",
    "\n",
    "The model builds a regression model to predict the probability that a given data entry belongs to the category numbered as “1”.\n",
    "Just like Linear regression assumes that the data follows a linear function, Logistic regression models the data using the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0772e6",
   "metadata": {},
   "source": [
    "Logistic regression becomes a classification technique only when a decision threshold is brought into the picture. The setting of the threshold value is a very important aspect of Logistic regression and is dependent on the classification problem itself.\n",
    "The decision for the value of the threshold value is majorly affected by the values of precision and recall. Ideally, we want both precision and recall to be 1, but this seldom is the case.\n",
    "In the case of a Precision-Recall tradeoff, we use the following arguments to decide upon the threshold:-\n",
    "1. Low Precision/High Recall: In applications where we want to reduce the number of false negatives without necessarily reducing the number of false positives, we choose a decision value that has a low value of Precision or a high value of Recall. For example, in a cancer diagnosis application, we do not want any affected patient to be classified as not affected without giving much heed to if the patient is being wrongfully diagnosed with cancer. This is because the absence of cancer can be detected by further medical diseases but the presence of the disease cannot be detected in an already rejected candidate.\n",
    "2. High Precision/Low Recall: In applications where we want to reduce the number of false positives without necessarily reducing the number of false negatives, we choose a decision value that has a high value of Precision or a low value of Recall. For example, if we are classifying customers whether they will react positively or negatively to a personalized advertisement, we want to be absolutely sure that the customer will react positively to the advertisement because otherwise, a negative reaction can cause a loss of potential sales from the customer.\n",
    "\n",
    "\n",
    "Based on the number of categories, Logistic regression can be classified as: \n",
    "\n",
    "    binomial: target variable can have only 2 possible types: “0” or “1” which may represent “win” vs “loss”, “pass” vs “fail”, “dead” vs “alive”, etc.\n",
    "    \n",
    "    multinomial: target variable can have 3 or more possible types which are not ordered(i.e. types have no quantitative significance) like “disease A” vs “disease B” vs “disease C”.\n",
    "    \n",
    "    ordinal: it deals with target variables with ordered categories. For example, a test score can be categorized as:“very poor”, “poor”, “good”, “very good”. Here, each category can be given a score like 0, 1, 2, 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8e5bfe",
   "metadata": {},
   "source": [
    "Formula is:\n",
    "    π= Pr(y=1/x)= (𝑒^β0+β1*x)/(1+𝑒^β0+β1*x)\n",
    "    \n",
    "    where Pr(y=1/x) repreents the probability of y=1 given x\n",
    "        (𝑒^β0+β1*x)/(1+𝑒^β0+β1*x) represents change in log odds for unit change in x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383dce8e",
   "metadata": {},
   "source": [
    "Odds is the ratio of probability of an event and probability of the event's complement\n",
    "    \n",
    "    Odds=π/(1-π)\n",
    "    \n",
    "    \n",
    "    logπ/(1-π)=log(𝑒^β0+β1*x)=β0+β1*x\n",
    "    The above equation states that thelog of odds is equal to linear regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaed881b",
   "metadata": {},
   "source": [
    "sklearn.linear_model.LogisticRegression(penalty='l2',dual=False,tol=0.0001,C=1.0,fit_intercept=True,intercept_scaling=1,\n",
    "\n",
    "class_weight=None,random_state=None,solver='liblinear',max_iter=100,multi_class='ovr',verbose=0,warm_start=False,n_jobs=1)\n",
    "\n",
    "    whereby: \n",
    "    penalty='l2' used to specify the norm used in penalization\n",
    "    dual=False it is implemented only for an L2 penalty\n",
    "    multi_class='ovr' means setting multiclass to one versus rest(ovr) which is suitable for binary problems. Can be set to ovr or multinomial\n",
    "    C=1.0 it is the inverse of regularization\n",
    "    fit_intercept=True calculates the intercept\n",
    "    warm_start=False  if it is true the algorithm reuses the solution of the previous call\n",
    "    random_state=None  it is the seed or the random state instance\n",
    "    n_jobs=1 specifies the number of jobs in parrallel computation\n",
    "    solver='liblinear is the algorithm to be used in the optimization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad337dad",
   "metadata": {},
   "source": [
    "# Logistic regression example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71184ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2d26c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef543bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris_dataset= load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd4412cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_columns=iris_dataset.feature_names\n",
    "iris_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f666ce31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iris_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "979bd7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_dataset.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8f35d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "700cf942",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7cc47d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_X=iris_dataset.data\n",
    "iris_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23d17fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_Y = iris_dataset.target\n",
    "iris_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a9fa92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 4), (150,))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_X.shape, iris_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f7f825ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "35ab7443",
   "metadata": {},
   "outputs": [],
   "source": [
    "logicreg=LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cd59544f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VICKFURY\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logicreg.fit(iris_X,iris_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aada63de",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data=[[3,5,4,1],[5,4,3,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66fc114f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VICKFURY\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logicreg.fit(iris_X,iris_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "84071357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logicreg.predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e84ec5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df668e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34c25e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "kneigbors=KNeighborsClassifier(n_neighbors=1)\n",
    "# Instantiating a class of a linear model is called an estimator\n",
    "# n_neighbors=1 means it will access its first nearest neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "217aaf62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kneigbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "30d69a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kneigbors.fit(iris_X,iris_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "75bbf7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data=[[3,5,4,1],[5,4,3,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e4813189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kneigbors.predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b0a2ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80d9236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cee1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43f9456c",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e17ae5e",
   "metadata": {},
   "source": [
    "It is used for both regression and classification problem types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456d0001",
   "metadata": {},
   "source": [
    "it uses the entire training dataset to create the entire model.\n",
    "It looks at the inputs of features of the training dataset to identify the attributes of any new unseen data\n",
    "Based on how similar a data point is to an attribute or in simpler terms how near it is to the attribute the algorithm classifies it\n",
    "\n",
    "If you are using this method for binary classification choose an odd number for k to avoid the case of a tied distance between 2 classes"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "318e348e",
   "metadata": {},
   "source": [
    "![image-2.png](attachment:image-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00e6251",
   "metadata": {},
   "source": [
    "    Example of k-NN classification.\n",
    "    The test sample (green dot) should be classified either to blue squares or to red triangles. \n",
    "    If k = 3 (solid line circle) it is assigned to the red triangles because there are 2 triangles and only 1 square inside the inner circle. \n",
    "    If k = 5 (dashed line circle) it is assigned to the blue squares (3 squares vs. 2 triangles inside the outer circle)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0ed469",
   "metadata": {},
   "source": [
    "# Unsupervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8807a279",
   "metadata": {},
   "source": [
    "## Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f73b2d",
   "metadata": {},
   "source": [
    "It is grouping of similar data points\n",
    "It is used in  extracting structure of data\n",
    "Identifies groupsin data\n",
    "\n",
    "Greater similarity in data points results in better clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba18b81",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb4eeb7",
   "metadata": {},
   "source": [
    "k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster\n",
    "\n",
    "    It finds the best centroids by assigning random centroids to a dataset\n",
    "    and selecting mean data points from the resulting clusters to form new centroids.\n",
    "    This process continues iteratively until model is optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770f8ee2",
   "metadata": {},
   "source": [
    "A data point is considered to be in a cluster if it is closer to the clusters centroid than any other centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9ce10a",
   "metadata": {},
   "source": [
    "K-means rndomly assigns specific number of centroids to a dataset. \n",
    "It assigns datapoints to centroids depending on their proximity to them \n",
    "Once the datapoints are grouped separately the algorithm chooses a datapoint that is in the center of each group\n",
    "then it reassigns data points to these new centroids based on their proximity\n",
    "If this creates a new set of groups and K-means again chooses a data point from the center of each group to form centroids and this process is iteratively done until model optimization is done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a808e099",
   "metadata": {},
   "source": [
    "### SKLEARN IMPORT K MEANS ALGORITHM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8891f3c1",
   "metadata": {},
   "source": [
    "    sklearn.cluster.KMeans(n_clusters=8,init='k-means++',n-init=10,max_iter=300,tol=0.0001,precompute_distances='auto',verbose=0,random_state=None,copy_X=True,n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e3aea1",
   "metadata": {},
   "source": [
    "    n_clusters=8 gives the number of clusters and centroids to be formed\n",
    "    init='k-means++' selects initial cluster centers\n",
    "    n-init=10 selects the number of times the K-means algorithm will be run with different centroid seeds\n",
    "    max_iter=300 specifies the maximum number of iterations of K-means algorithm for a single run\n",
    "    precompute_distances='auto' it is used to pre compute the algorithm for faster operation\n",
    "    random_state=None initialises the centers\n",
    "    copy_X=True this will not modify the data while the algorithm carries out precomputation\n",
    "    n_jobs=1 specifies the number of jobs in parallel computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5793d81",
   "metadata": {},
   "source": [
    "## KMEANSDEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "725936d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "be324d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "41b1da88",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans=KMeans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a1f0c384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets._samples_generator.make_blobs(n_samples=100, n_features=2, *, centers=None, cluster_std=1.0, center_box=(-10.0, 10.0), shuffle=True, random_state=None, return_centers=False)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "459ceb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=20\n",
    "n_samples=300\n",
    "X,y=make_blobs(n_samples=n_samples,n_features=5,random_state=None)\n",
    "predict_y=KMeans(n_clusters=3,random_state=random_state).fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "32b867b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.45149317,  1.53134442,  2.27157419,  5.95372154,  3.3742307 ],\n",
       "       [-1.86429256,  3.14501995,  0.69748925,  7.13784913,  6.13622162],\n",
       "       [-3.00257496,  3.32977232,  1.39458246,  6.09088181,  4.33305052],\n",
       "       ...,\n",
       "       [-1.28242108, -8.67848736, -4.31000965,  9.8331587 ,  4.63230541],\n",
       "       [ 3.69008191, -5.64531323,  9.67291679, -8.74534344,  6.71558011],\n",
       "       [-2.20241395,  2.37685677,  3.28456513,  5.33413421,  3.87874842]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3ff10f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 2, 1, 1, 2, 2, 0, 1, 0, 1, 0, 2, 2, 1, 0, 1, 1, 2, 1, 0,\n",
       "       2, 2, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 2, 2, 0, 1, 0, 0, 1, 0, 1,\n",
       "       2, 0, 2, 2, 1, 2, 0, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 0, 2,\n",
       "       1, 0, 2, 1, 1, 2, 2, 1, 0, 2, 0, 0, 1, 1, 2, 0, 2, 0, 0, 0, 0, 2,\n",
       "       1, 2, 2, 1, 2, 2, 0, 0, 0, 0, 2, 1, 1, 1, 2, 2, 1, 0, 0, 0, 2, 2,\n",
       "       2, 2, 0, 2, 1, 0, 1, 1, 1, 0, 0, 2, 0, 1, 0, 0, 0, 2, 2, 2, 2, 0,\n",
       "       2, 1, 0, 1, 0, 1, 2, 0, 1, 0, 0, 0, 1, 1, 2, 0, 1, 2, 1, 2, 2, 1,\n",
       "       0, 2, 0, 2, 1, 2, 2, 1, 1, 0, 0, 2, 1, 2, 0, 1, 1, 2, 0, 0, 2, 1,\n",
       "       2, 1, 2, 0, 1, 2, 1, 1, 2, 1, 0, 1, 0, 2, 2, 1, 0, 2, 0, 0, 1, 2,\n",
       "       2, 0, 0, 0, 2, 1, 1, 0, 2, 2, 1, 0, 2, 0, 1, 2, 1, 1, 1, 2, 0, 1,\n",
       "       1, 1, 2, 0, 2, 0, 1, 1, 2, 1, 1, 2, 2, 0, 0, 0, 1, 2, 0, 0, 1, 2,\n",
       "       2, 0, 0, 2, 2, 2, 2, 0, 2, 1, 2, 2, 1, 0, 1, 2, 0, 2, 0, 1, 2, 0,\n",
       "       0, 2, 2, 1, 0, 1, 1, 0, 0, 1, 1, 2, 2, 0, 1, 1, 0, 2, 2, 0, 2, 0,\n",
       "       1, 0, 1, 0, 2, 0, 1, 0, 0, 1, 0, 1, 2, 0])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0fa30eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 2, 2, 1, 1, 0, 2, 0, 2, 0, 1, 1, 2, 0, 2, 2, 1, 2, 0,\n",
       "       1, 1, 2, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 1, 1, 0, 2, 0, 0, 2, 0, 2,\n",
       "       1, 0, 1, 1, 2, 1, 0, 1, 1, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 2, 0, 1,\n",
       "       2, 0, 1, 2, 2, 1, 1, 2, 0, 1, 0, 0, 2, 2, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "       2, 1, 1, 2, 1, 1, 0, 0, 0, 0, 1, 2, 2, 2, 1, 1, 2, 0, 0, 0, 1, 1,\n",
       "       1, 1, 0, 1, 2, 0, 2, 2, 2, 0, 0, 1, 0, 2, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "       1, 2, 0, 2, 0, 2, 1, 0, 2, 0, 0, 0, 2, 2, 1, 0, 2, 1, 2, 1, 1, 2,\n",
       "       0, 1, 0, 1, 2, 1, 1, 2, 2, 0, 0, 1, 2, 1, 0, 2, 2, 1, 0, 0, 1, 2,\n",
       "       1, 2, 1, 0, 2, 1, 2, 2, 1, 2, 0, 2, 0, 1, 1, 2, 0, 1, 0, 0, 2, 1,\n",
       "       1, 0, 0, 0, 1, 2, 2, 0, 1, 1, 2, 0, 1, 0, 2, 1, 2, 2, 2, 1, 0, 2,\n",
       "       2, 2, 1, 0, 1, 0, 2, 2, 1, 2, 2, 1, 1, 0, 0, 0, 2, 1, 0, 0, 2, 1,\n",
       "       1, 0, 0, 1, 1, 1, 1, 0, 1, 2, 1, 1, 2, 0, 2, 1, 0, 1, 0, 2, 1, 0,\n",
       "       0, 1, 1, 2, 0, 2, 2, 0, 0, 2, 2, 1, 1, 0, 2, 2, 0, 1, 1, 0, 1, 0,\n",
       "       2, 0, 2, 0, 1, 0, 2, 0, 0, 2, 0, 2, 1, 0])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1dc554",
   "metadata": {},
   "source": [
    "## DIMENSIONALITY REDUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91758bfe",
   "metadata": {},
   "source": [
    "It reduces a high dimensional dataset into fewer dimensions making it easier and faster for algorithm to analyse data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51db9942",
   "metadata": {},
   "source": [
    "It is used when u have huge amounts of data that would cause slow performance during analysis example:a spreadsheet with 1000 rows and 1000 columns\n",
    "For this you have to trim down the spreadsheet for faster computation but retain the info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acc6c91",
   "metadata": {},
   "source": [
    "### Techniques of dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b395fa3a",
   "metadata": {},
   "source": [
    "    . Drop columns with missing values and exceed some threshold value for the dataset\n",
    "    . Drop data columns with low variance(Data variance indicates changes in the data value.If there are small changes in data then you can remove data below threshold after normalizing it)\n",
    "    . Drop data columns with high correlations. \n",
    "    . Apply Principal Component Analysis which transforms original datasets into a new set of coordinates by keeping the highest possible variants to ensure loss in data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441c9fec",
   "metadata": {},
   "source": [
    "### Principal component analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84441bc",
   "metadata": {},
   "source": [
    " Principal component analysis (PCA) is the process of computing the principal components and using them to perform a change of basis on the data, sometimes using only the first few principal components and ignoring the rest.\n",
    " \n",
    "It is a statistical procedure that uses an orthogonal transformation that converts a set of correlated variables to a set of uncorrelated variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d531dffe",
   "metadata": {},
   "source": [
    "PCA is an unsupervised statistical technique used to examine the interrelations among a set of variables. It is also known as a general factor analysis where regression determines a line of best fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf29dddc",
   "metadata": {},
   "source": [
    "It is a linear dimensionality reduction method which uses singular value decomposition of data and keeps only most significant singular vectors to project data to lower dimensional space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f35eb0",
   "metadata": {},
   "source": [
    "Used to reduce data. It tries to capture variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138f76aa",
   "metadata": {},
   "source": [
    "### PCA IN SCIKIT-LEARN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1283c211",
   "metadata": {},
   "source": [
    "sklearn.decomposition.PCA(n_components=None,copy=True,whiten=False)\n",
    "\n",
    "    . n_components=None indicates the number of components that should be retained.If it is not set then all components are retained\n",
    "    . copy=True  overwrite the transform data after fitting it into the model. It is not used frequently because we dont often need to overwrite transform data\n",
    "    . whiten=False removes data with lower variance and improves accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b019c15f",
   "metadata": {},
   "source": [
    "### PCA DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bac9d104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "498b8e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample=20\n",
    "random_state=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9a2c8eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=make_blobs(n_samples=n_sample,n_features=10,random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b2bc4760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 10)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4fce5f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca=PCA(n_components=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "63b4c11d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.74041033, 0.23090862, 0.00768222])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(X)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e9c9f0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06739726, -0.41807217, -0.45275214,  0.20606586, -0.50029809,\n",
       "       -0.33701116, -0.24938606, -0.33622208,  0.12980151, -0.1313918 ])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first pca component\n",
    "first_pca=pca.components_[0]\n",
    "first_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8dee2524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using transform method to apply the reduction to fitted data\n",
    "pca_reduced=pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9383e47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 16.27547381,  -5.68444035,   2.76618587],\n",
       "       [ -1.4428934 ,   9.39855109,   1.84427654],\n",
       "       [-16.06659084,  -6.92891029,   0.88728361],\n",
       "       [-15.384256  ,  -6.03425009,  -1.87913145],\n",
       "       [ 15.33275762,  -3.54575315,  -0.94166925],\n",
       "       [ 16.78811138,  -4.81101274,  -1.6116703 ],\n",
       "       [ 16.27497794,  -4.9178648 ,  -0.23367594],\n",
       "       [ -1.37762721,  10.73706692,  -0.3125697 ],\n",
       "       [-13.94046855,  -5.53885218,  -0.69966986],\n",
       "       [ -1.43523822,  11.04811725,  -0.18304399],\n",
       "       [ 17.23607113,  -4.81446405,   1.58640229],\n",
       "       [-14.54478005,  -5.88796375,  -0.46531328],\n",
       "       [ 15.44807081,  -3.02870774,  -1.21720537],\n",
       "       [ -1.61156056,  12.16890568,   0.6632369 ],\n",
       "       [-16.52480762,  -5.20242534,  -1.32355209],\n",
       "       [-14.41464687,  -3.84818653,   0.82403914],\n",
       "       [-15.19082108,  -3.94578018,   2.5387103 ],\n",
       "       [ -1.60482354,  12.1888339 ,  -1.63688157],\n",
       "       [ -0.44913493,  11.2034045 ,   0.02230143],\n",
       "       [ 16.63218616,  -2.55626813,  -0.62805328]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bb78bb61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 3)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f47d20",
   "metadata": {},
   "source": [
    "# PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cd3c36",
   "metadata": {},
   "source": [
    "Used to combine multiple models or estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc929d48",
   "metadata": {},
   "source": [
    "## CHARACTERISTICS OF PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99715cd",
   "metadata": {},
   "source": [
    "    . Simplifies the process where more than one model is required or used\n",
    "    . Once all data is fit into the models or estimators the predict method can be called\n",
    "    . All models in the pipeline must be transformers. The last model can either be a transformer or a classifier, regressor or other such objects\n",
    " \n",
    "A pipeline is a combiantion of vectorizers, transformers(tf-idf) and model training\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb449a83",
   "metadata": {},
   "source": [
    "## PIPELINE DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a0c1340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT PIPELINE CLASS\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d74c258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3a8e7592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chaining the estimators together\n",
    "chained_estimators=[(\"dim_reduction\",PCA()),(\"linear_model\",LinearRegression())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2072a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the chained estimators in a pipeline object\n",
    "pipeline_estimator=Pipeline(chained_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "77d16f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('dim_reduction', PCA()), ('linear_model', LinearRegression())])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check chain of estimators\n",
    "pipeline_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "08f1af01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('dim_reduction', PCA())"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the first step\n",
    "pipeline_estimator.steps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ba655324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('linear_model', LinearRegression())"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the second step\n",
    "pipeline_estimator.steps[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4d60afa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dim_reduction', PCA()), ('linear_model', LinearRegression())]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View all steps in pipeline\n",
    "pipeline_estimator.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf6718a",
   "metadata": {},
   "source": [
    "## MODEL PERSISTENCE AND EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45d92d6",
   "metadata": {},
   "source": [
    "MODEL PERSISTENCE is the act of saving ur model for future use since there is no need to retrain your model every time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92fdf5d",
   "metadata": {},
   "source": [
    "This is provided by pickle method in python\n",
    "Sckit learn has a special replacement for pickle called joblib\n",
    "\n",
    "You can use joblib.dump, joblib.load methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5ae67c",
   "metadata": {},
   "source": [
    "### MODEL PERSISTENCE AND EVALUATION DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2f5d6934",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris_dataset=load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "51d4c10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_dataset.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "83327634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0c71aec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_X=iris_dataset.data\n",
    "iris_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "012156de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_Y=iris_dataset.target\n",
    "iris_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b154fd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new=[[3,5,4,1],[5,4,3,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0a1aa92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg=LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "79b5c6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VICKFURY\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(iris_X,iris_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5d5a0653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "571b35ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c3ab0b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x80\\x04\\x95\\x08\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x1esklearn.linear_model._logistic\\x94\\x8c\\x12LogisticRegression\\x94\\x93\\x94)\\x81\\x94}\\x94(\\x8c\\x07penalty\\x94\\x8c\\x02l2\\x94\\x8c\\x04dual\\x94\\x89\\x8c\\x03tol\\x94G?\\x1a6\\xe2\\xeb\\x1cC-\\x8c\\x01C\\x94G?\\xf0\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\rfit_intercept\\x94\\x88\\x8c\\x11intercept_scaling\\x94K\\x01\\x8c\\x0cclass_weight\\x94N\\x8c\\x0crandom_state\\x94N\\x8c\\x06solver\\x94\\x8c\\x05lbfgs\\x94\\x8c\\x08max_iter\\x94Kd\\x8c\\x0bmulti_class\\x94\\x8c\\x04auto\\x94\\x8c\\x07verbose\\x94K\\x00\\x8c\\nwarm_start\\x94\\x89\\x8c\\x06n_jobs\\x94N\\x8c\\x08l1_ratio\\x94N\\x8c\\x0en_features_in_\\x94K\\x04\\x8c\\x08classes_\\x94\\x8c\\x15numpy.core.multiarray\\x94\\x8c\\x0c_reconstruct\\x94\\x93\\x94\\x8c\\x05numpy\\x94\\x8c\\x07ndarray\\x94\\x93\\x94K\\x00\\x85\\x94C\\x01b\\x94\\x87\\x94R\\x94(K\\x01K\\x03\\x85\\x94h\\x1c\\x8c\\x05dtype\\x94\\x93\\x94\\x8c\\x02i4\\x94K\\x00K\\x01\\x87\\x94R\\x94(K\\x03\\x8c\\x01<\\x94NNNJ\\xff\\xff\\xff\\xffJ\\xff\\xff\\xff\\xffK\\x00t\\x94b\\x89C\\x0c\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x94t\\x94b\\x8c\\x07n_iter_\\x94h\\x1bh\\x1eK\\x00\\x85\\x94h \\x87\\x94R\\x94(K\\x01K\\x01\\x85\\x94h(\\x89C\\x04d\\x00\\x00\\x00\\x94t\\x94b\\x8c\\x05coef_\\x94h\\x1bh\\x1eK\\x00\\x85\\x94h \\x87\\x94R\\x94(K\\x01K\\x03K\\x04\\x86\\x94h%\\x8c\\x02f8\\x94K\\x00K\\x01\\x87\\x94R\\x94(K\\x03h)NNNJ\\xff\\xff\\xff\\xffJ\\xff\\xff\\xff\\xffK\\x00t\\x94b\\x89C`e\\xf4\\xf8\\xc5\\xd3\\xbf\\xda\\xbf\\xd4\\xb3*\\x16\\xff\\xea\\xee?I\\xaf\\xde!\\xdb+\\x04\\xc0\\x89k&\\n\\x1aX\\xf1\\xbf\\xa6\\xd7\\xcb\\x8b\\xc7\\xfb\\xe0?`\\xbb(\\xc7\\xd2\\x1e\\xd4\\xbf\\xef\\xc0\\xeb\\xa7g|\\xc9\\xbf\\x141Mf\\xa2]\\xee\\xbfP\\xe0zF\\xed\\xde\\xbc\\xbfeU\\x96\\xb2\\x95\\xdb\\xe4\\xbf\\xb7k]\\x9c\\xa1\\xc3\\x05@\"\\x82\\xa6\\x9euC\\x00@\\x94t\\x94b\\x8c\\nintercept_\\x94h\\x1bh\\x1eK\\x00\\x85\\x94h \\x87\\x94R\\x94(K\\x01K\\x03\\x85\\x94h;\\x89C\\x18#q`\\r1\\xad#@:\\xdb\\xd2w\\x8e\\xb8\\x01@o(U\\xabT\\x1b(\\xc0\\x94t\\x94b\\x8c\\x10_sklearn_version\\x94\\x8c\\x060.24.1\\x94ub.'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "savedmodel=pk.dumps(logreg)\n",
    "savedmodel\n",
    "# Use the dumps method to persist the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0f402d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use joblib.dump to persist model to a file\n",
    "# from sklearn.externals import joblib\n",
    "import joblib\n",
    "savedmodel=joblib.dump(logreg,'regressionmodel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "30e7d79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use joblib.load to persist model to a file \n",
    "newlogregestimator=joblib.load('regressionmodel.pkl')\n",
    "newlogregestimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f516d2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newlogregestimator.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e893ac",
   "metadata": {},
   "source": [
    "## Metric Functions for model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287f4f9d",
   "metadata": {},
   "source": [
    "The metrics function in scikit learn can be used to evaluate quality of classification, clustering and regression type models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce99ae53",
   "metadata": {},
   "source": [
    "To check quality of classification type we use metrics such as:\n",
    "\n",
    "    accuracy_score\n",
    "    average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c69d120",
   "metadata": {},
   "source": [
    "To check quality of clustering type we use metrics such as:\n",
    "    \n",
    "    adjusted_rand_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01312d37",
   "metadata": {},
   "source": [
    "To check quality of regression type we use metrics such as:\n",
    "    \n",
    "    mean_absolute_error\n",
    "    mean_squared_error\n",
    "    median_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87689674",
   "metadata": {},
   "source": [
    "Supervised machine learning can be used to predict outcome of a dataset while unsupervised machine learning can be used to obtain structure of a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16d98d5",
   "metadata": {},
   "source": [
    "## NATURAL LANGUAGE PROCESSING (NLP) WITH SCIKIT-LEARN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da50f77",
   "metadata": {},
   "source": [
    "NLP is a way to understand and analyze natural human languages \n",
    "and extract info from such data by applying machine learning algorithms\n",
    "Such data can be in form of video, audio or text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65eec34",
   "metadata": {},
   "source": [
    "In NLP full automation can be achieved using modern software libraries, modules and packages\n",
    "\n",
    "Corpus is a collection of machine readable texts produced in a natural communicative setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e44aae",
   "metadata": {},
   "source": [
    "## NLP Terminologies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab5fa89",
   "metadata": {},
   "source": [
    "    . Word boundaries -Determines where one word ends and the other begins\n",
    "    . Tokenization - It is a technique to split words, phrases and idioms into smaller units called tokens\n",
    "    . Stemming - Maps word to the valid root word. Useful in finding synonyms and are used in search engines. Iit is the process of reducing a word to its stem/extracting the base form of words by removing affixes from them. For example: the stem of words eating, eats, eaten is eat \n",
    "    . Tf-Idf -Represents Term Frequency and inverse document frequency. It is a numerical value which represents how important  word is to a corpus or how relevant a string is depending on how many times it occurs in  a corpus or a document.\n",
    "    . Semantic analysis -Compares words, phrases and idioms in a set of documents to extract meaning\n",
    "    . Disambiguation - Determines meanng and sense of words ie:context vs intent\n",
    "    . Topic models -Discovers topics in a collection of documents\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829f8023",
   "metadata": {},
   "source": [
    "## NLP Approaches to analyze text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cb68e5",
   "metadata": {},
   "source": [
    "    . Basic text processing- extracting keywords\n",
    "    . Categorizing and tagging words- words can be categorized based on maybe if they are verbs, adjectives or nouns\n",
    "    . classify text- identfy features of language and use them to classify. for example sports, religious\n",
    "    . extract info- identfy entities and relationships in text.\n",
    "    . analyzr sentence structure\n",
    "    . build feature based structure\n",
    "    . analyze meaning -perform quantitative analysis of text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65428b73",
   "metadata": {},
   "source": [
    "### INSTALLING NLP ENVIRONMENT IN ANACONDA PROMPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4634f1ac",
   "metadata": {},
   "source": [
    "    conda install scikit-learn \n",
    "    conda install nltk\n",
    "   To install the necessary corpus do the following:\n",
    "   \n",
    "       python\n",
    "       import nltk\n",
    "       nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690f2e69",
   "metadata": {},
   "source": [
    "### SENETENCE ANALYSIS DEMO\n",
    "Eliminate punctuation and stopwords from sentences\n",
    "\n",
    "stopwords are words which do not add value to a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "652751f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d8e67325",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View first 10 stopwords in english corpus\n",
    "stopwords.words('english')[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "200e4993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test sentence\n",
    "test_sentence='This is my first test string. Wow!! we are doing just fine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7b7b0367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'm',\n",
       " 'y',\n",
       " ' ',\n",
       " 'f',\n",
       " 'i',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " ' ',\n",
       " 't',\n",
       " 'e',\n",
       " 's',\n",
       " 't',\n",
       " ' ',\n",
       " 's',\n",
       " 't',\n",
       " 'r',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'W',\n",
       " 'o',\n",
       " 'w',\n",
       " ' ',\n",
       " 'w',\n",
       " 'e',\n",
       " ' ',\n",
       " 'a',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'd',\n",
       " 'o',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'j',\n",
       " 'u',\n",
       " 's',\n",
       " 't',\n",
       " ' ',\n",
       " 'f',\n",
       " 'i',\n",
       " 'n',\n",
       " 'e']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eliminate punctuation in form of characters and print them\n",
    "non_punctuated=[char for char in test_sentence if char not in string.punctuation]\n",
    "non_punctuated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2db16684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is my first test string Wow we are doing just fine'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eliminate punctuation and print them as a whole sentence\n",
    "non_punctuated=''.join(non_punctuated)\n",
    "non_punctuated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d5e29a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'my',\n",
       " 'first',\n",
       " 'test',\n",
       " 'string',\n",
       " 'Wow',\n",
       " 'we',\n",
       " 'are',\n",
       " 'doing',\n",
       " 'just',\n",
       " 'fine']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split each words present in the new sentence\n",
    "non_punctuated.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dd5abb3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first', 'test', 'string', 'Wow', 'fine']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eliminate stopwords\n",
    "cleaned_sentence=[word for word in non_punctuated.split() if word.lower() not in stopwords.words('english')]\n",
    "cleaned_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a4daab",
   "metadata": {},
   "source": [
    "# Applications of NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2462e7",
   "metadata": {},
   "source": [
    "Machine Translation \n",
    "    Translating one language to another eg: Google translate\n",
    "    \n",
    "Speech recogntion \n",
    "    understands human speech and uses it as input info. It is useful for apps such as: Siri, Alexa\n",
    "    \n",
    "Sentimental Analysis\n",
    "        It is achieved by processing tons of data received from different interfaces and sources. eg: use of NLP to find out popular topic of discussion or importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81b3cb4",
   "metadata": {},
   "source": [
    "### NLP Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35701b78",
   "metadata": {},
   "source": [
    "    NLTK\n",
    "    Sci-kit learn\n",
    "    TextBlob\n",
    "    Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641986c0",
   "metadata": {},
   "source": [
    "## Scikit learn Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d7e0ac",
   "metadata": {},
   "source": [
    "### Features of sci-kit learn\n",
    "\n",
    "    Built in modules\n",
    "    Feature extraction\n",
    "    Model training\n",
    "    Pipeline building mechanism\n",
    "    Performance optimization\n",
    "    Grid search for finding good parameters affecting outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37154c4",
   "metadata": {},
   "source": [
    "### Stages of pipeline learning\n",
    "    Vectorization- Converting textual documents into numerical vectors.\n",
    "    Documents are split into words or tokens and each document is assigned a number.\n",
    "    \n",
    "    Transformation- Extracting features around the word of interest. You can also find occurrence of each word \n",
    "    \n",
    "    Model training and application- Model is divided into train and testing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fdd7ee",
   "metadata": {},
   "source": [
    "### Modules to load content and category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eae601",
   "metadata": {},
   "source": [
    "There are several methods to load datasets with the help of data load objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a496f4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "497ac27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets._base.load_files(container_path, *, description=None, categories=None, load_content=True, shuffle=True, encoding=None, decode_error='strict', random_state=0)>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_files=sklearn.datasets.load_files\n",
    "load_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537874db",
   "metadata": {},
   "source": [
    "To use text files in scikitlearn classification or clustering algorithm use the following:\n",
    "\n",
    "    from sklearn.feature_extraction.text import "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e82575c",
   "metadata": {},
   "source": [
    "#### Attribute of data load object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361ee92e",
   "metadata": {},
   "source": [
    "Bunch - Contains fields and can be accessed as dictionary keys or an object\n",
    "\n",
    "Target Names - list of requested categories\n",
    "\n",
    "Data - Refers to an attribute in memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9be8fb2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "        [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "        [ 0.,  0., 10., ..., 12.,  1.,  0.]]),\n",
       " 'target': array([0, 1, 2, ..., 8, 9, 8]),\n",
       " 'frame': None,\n",
       " 'feature_names': ['pixel_0_0',\n",
       "  'pixel_0_1',\n",
       "  'pixel_0_2',\n",
       "  'pixel_0_3',\n",
       "  'pixel_0_4',\n",
       "  'pixel_0_5',\n",
       "  'pixel_0_6',\n",
       "  'pixel_0_7',\n",
       "  'pixel_1_0',\n",
       "  'pixel_1_1',\n",
       "  'pixel_1_2',\n",
       "  'pixel_1_3',\n",
       "  'pixel_1_4',\n",
       "  'pixel_1_5',\n",
       "  'pixel_1_6',\n",
       "  'pixel_1_7',\n",
       "  'pixel_2_0',\n",
       "  'pixel_2_1',\n",
       "  'pixel_2_2',\n",
       "  'pixel_2_3',\n",
       "  'pixel_2_4',\n",
       "  'pixel_2_5',\n",
       "  'pixel_2_6',\n",
       "  'pixel_2_7',\n",
       "  'pixel_3_0',\n",
       "  'pixel_3_1',\n",
       "  'pixel_3_2',\n",
       "  'pixel_3_3',\n",
       "  'pixel_3_4',\n",
       "  'pixel_3_5',\n",
       "  'pixel_3_6',\n",
       "  'pixel_3_7',\n",
       "  'pixel_4_0',\n",
       "  'pixel_4_1',\n",
       "  'pixel_4_2',\n",
       "  'pixel_4_3',\n",
       "  'pixel_4_4',\n",
       "  'pixel_4_5',\n",
       "  'pixel_4_6',\n",
       "  'pixel_4_7',\n",
       "  'pixel_5_0',\n",
       "  'pixel_5_1',\n",
       "  'pixel_5_2',\n",
       "  'pixel_5_3',\n",
       "  'pixel_5_4',\n",
       "  'pixel_5_5',\n",
       "  'pixel_5_6',\n",
       "  'pixel_5_7',\n",
       "  'pixel_6_0',\n",
       "  'pixel_6_1',\n",
       "  'pixel_6_2',\n",
       "  'pixel_6_3',\n",
       "  'pixel_6_4',\n",
       "  'pixel_6_5',\n",
       "  'pixel_6_6',\n",
       "  'pixel_6_7',\n",
       "  'pixel_7_0',\n",
       "  'pixel_7_1',\n",
       "  'pixel_7_2',\n",
       "  'pixel_7_3',\n",
       "  'pixel_7_4',\n",
       "  'pixel_7_5',\n",
       "  'pixel_7_6',\n",
       "  'pixel_7_7'],\n",
       " 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " 'images': array([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ..., 15.,  5.,  0.],\n",
       "         [ 0.,  3., 15., ..., 11.,  8.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 11., ..., 12.,  7.,  0.],\n",
       "         [ 0.,  2., 14., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n",
       "         [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  9., 16., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  3., 13., ..., 11.,  5.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ...,  2.,  1.,  0.],\n",
       "         [ 0.,  0., 16., ..., 16.,  5.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0., 16., ..., 15.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 16.,  0.,  0.],\n",
       "         [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0., 14., ..., 15.,  1.,  0.],\n",
       "         [ 0.,  4., 16., ..., 16.,  7.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  2., 16., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 15.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 16., ..., 16.,  6.,  0.],\n",
       "         [ 0.,  8., 16., ..., 16.,  8.,  0.],\n",
       "         [ 0.,  1.,  8., ..., 12.,  1.,  0.]]]),\n",
       " 'DESCR': \".. _digits_dataset:\\n\\nOptical recognition of handwritten digits dataset\\n--------------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 1797\\n    :Number of Attributes: 64\\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n    :Missing Attribute Values: None\\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n    :Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\n.. topic:: References\\n\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\\n\"}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digit_dataset=load_digits()\n",
    "digit_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "77eec2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\".. _digits_dataset:\\n\\nOptical recognition of handwritten digits dataset\\n--------------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 1797\\n    :Number of Attributes: 64\\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n    :Missing Attribute Values: None\\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n    :Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\n.. topic:: References\\n\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\\n\""
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit_dataset.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4542e1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(digit_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "183fd3d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "       ...,\n",
       "       [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "       [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "       [ 0.,  0., 10., ..., 12.,  1.,  0.]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit_dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "33b0a554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 8, 9, 8])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit_dataset.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d971d7c",
   "metadata": {},
   "source": [
    "## FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bebfb8d",
   "metadata": {},
   "source": [
    "It is a technique to convert content into numerical vectors to perform machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a2329f",
   "metadata": {},
   "source": [
    "### TYPES OF FEATURE EXTRACTION "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728e3161",
   "metadata": {},
   "source": [
    "    Text feature extraction -large datasets or documents \n",
    "    Image feature extraction -patch extraction, hierachical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e58cf0e",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab17b18",
   "metadata": {},
   "source": [
    "It is used to convert text data into numerical feature vectors with a fixed size\n",
    "\n",
    "Steps involved:\n",
    "\n",
    "    Tokenizing- Assign a fixed integer id to each word\n",
    "    Counting - Number of occurrences of each word\n",
    "    Store - Store as value feature in matrix format\n",
    "Countvectorizer class signature \n",
    "\n",
    "     CountVectorizer(\n",
    "     input='content',encoding='utf-8',\n",
    "     decode_error='strict',strip_accents=None,\n",
    "     lowercase=True,preprocessor=None,\n",
    "     tokenizer=None, stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b',\n",
    "     ngram_range=(1,1),analyzer='word',max_df=1.0,min_df=1,\n",
    "     max_features=None,vocabulary=None,\n",
    "     binary=False,dtype=<class'numpy.int64'>\n",
    "     )   \n",
    "  where:\n",
    "  \n",
    "        CountVectorizer- specifies number of components to keep\n",
    "        input='content'- Filename sequence of strings\n",
    "        encoding='utf-8' specifies the encoding used to decode input\n",
    "        decode_error='strict'-  \n",
    "        strip_accents=None - remove accents\n",
    "        lowercase=True\n",
    "        preprocessor=None\n",
    "        tokenizer=None - overrides string tokenizer method\n",
    "        stop_words=None - Built in stopwords list used\n",
    "        token_pattern='(?u)\\b\\w\\w+\\b'\n",
    "        ngram_range=(1,1)\n",
    "        analyzer='word'\n",
    "        max_df=1.0 - Max threshold - Indicates terms or wors that appear more than a given threshold value and should be ignored\n",
    "        min_df=1 - Min threshold - Indicates tems or words that appear less than a given threshold value\n",
    "        max_features=None\n",
    "        vocabulary=None\n",
    "        binary=False\n",
    "        dtype=<class'numpy.int64'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cb3553",
   "metadata": {},
   "source": [
    "### BAG OF WORDS DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "be25f883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ecfbef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer= CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "006c5241",
   "metadata": {},
   "outputs": [],
   "source": [
    "document1='Hi How are you'\n",
    "document2='today is a very very pleasant day and we can have some fun fun fun'\n",
    "document3='This was an amazing experience'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "00d61944",
   "metadata": {},
   "outputs": [],
   "source": [
    "doclist=[document1,document2,document3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b88b989a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words=vectorizer.fit(doclist)\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c6eb7434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x20 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 20 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words=vectorizer.transform(doclist)\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bd684429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of bag of words is a tuple with 2 elements where the first element is a document number \n",
    "# while the second is the feature indices of each word and a number which is the frequency of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4913a4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the vocabulary for the repeated word\n",
    "vectorizer.vocabulary_.get('very')\n",
    "vectorizer.vocabulary_.get('fun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "516f4cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712a0b6e",
   "metadata": {},
   "source": [
    "## Text feature extraction considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec48da19",
   "metadata": {},
   "source": [
    "Sparse -Used when dealing with sparse matrix while storing them in memory\n",
    "\n",
    "Vectorizer - Implements tokenization and occurrence\n",
    "\n",
    "Tf Idf- \n",
    "\n",
    "Decoding- This utility can decode text files if their decoding is specified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d538534",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1fc0c5",
   "metadata": {},
   "source": [
    "It is a basic technique for text classification \n",
    "\n",
    "It is based on the Bayes theorem\n",
    "It assumes that the probability of each attribute belongs to a specific class value and is independent of other attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7432d30d",
   "metadata": {},
   "source": [
    "Advantages\n",
    "\n",
    "    Efficient as it uses limited CPU and memory\n",
    "    Fast as the model training takes less time\n",
    "\n",
    "Uses\n",
    "\n",
    "    Sentimental analysis, email spam detection,categorization of documents, language detection\n",
    "    Multinomial Naive Bayes is used when multiple occurrences of words matter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957c2024",
   "metadata": {},
   "source": [
    "sklearn.naive_bayes.MultinomialNB(alpha=1.0,fit_prior=True,class_prior=None)\n",
    "\n",
    "    alpha=1.0 - Smoothing parameter (0 for no smoothing)\n",
    "    fit_prior=True - Indicates whether to learn class prior probabilities\n",
    "    class_prior=None - Specifies prior probabilities of the classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784ebdda",
   "metadata": {},
   "source": [
    "## GRID SEARCH AND MULTIPLE PARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a916c42a",
   "metadata": {},
   "source": [
    "Document classifiers can have many parameters and a Grid approach helps to search the best parameters for model training and predicting the outcome accurately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea207c4b",
   "metadata": {},
   "source": [
    "In grid search mechanism, the whole dataset can be divided into multiple grids and a search can be run on multiple grids \n",
    "or a combination of grids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca3e737",
   "metadata": {},
   "source": [
    "CONSTRAINTS OF GRID SEARCH\n",
    "     \n",
    "    CPU memory constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdc7a98",
   "metadata": {},
   "source": [
    "## GRID SEARCH DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8a1d8df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from pprint import pprint\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d174c22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_collection=pd.read_csv('C:/Users/VICKFURY/Documents/projects/Python Scripts/ml/datasets/spam.csv', sep=',')\n",
    "# sep is used for separating the commas in the document since it is a csv file\n",
    "# spam_collection=pd.read_excel('C:/Users/VICKFURY/Documents/projects/Python Scripts/ml/datasets/spam.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bfa06b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Response</th>\n",
       "      <th>Message</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will �_ b going to esplanade fr home?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Response                                            Message Unnamed: 2  \\\n",
       "0         ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1         ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2        spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3         ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4         ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "...       ...                                                ...        ...   \n",
       "5567     spam  This is the 2nd time we have tried 2 contact u...        NaN   \n",
       "5568      ham              Will �_ b going to esplanade fr home?        NaN   \n",
       "5569      ham  Pity, * was in mood for that. So...any other s...        NaN   \n",
       "5570      ham  The guy did some bitching but I acted like i'd...        NaN   \n",
       "5571      ham                         Rofl. Its true to its name        NaN   \n",
       "\n",
       "     Unnamed: 3  \n",
       "0           NaN  \n",
       "1           NaN  \n",
       "2           NaN  \n",
       "3           NaN  \n",
       "4           NaN  \n",
       "...         ...  \n",
       "5567        NaN  \n",
       "5568        NaN  \n",
       "5569        NaN  \n",
       "5570        NaN  \n",
       "5571        NaN  \n",
       "\n",
       "[5572 rows x 4 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7d13922a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Response', 'Message', 'Unnamed: 2', 'Unnamed: 3'], dtype='object')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_collection.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "95c99f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Response</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will �_ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Response                                            Message\n",
       "0         ham  Go until jurong point, crazy.. Available only ...\n",
       "1         ham                      Ok lar... Joking wif u oni...\n",
       "2        spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3         ham  U dun say so early hor... U c already then say...\n",
       "4         ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...       ...                                                ...\n",
       "5567     spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568      ham              Will �_ b going to esplanade fr home?\n",
       "5569      ham  Pity, * was in mood for that. So...any other s...\n",
       "5570      ham  The guy did some bitching but I acted like i'd...\n",
       "5571      ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_collection.drop(['Unnamed: 2', 'Unnamed: 3'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "60114dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e89fea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SGD classifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "57a3f121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import gridsearch\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ac98b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline=Pipeline([('vect',CountVectorizer()),('tfidf',TfidfTransformer()),('clasifier',SGDClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "516495b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for grid search\n",
    "parameters={'tfidf__use_idf':(True,False)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "12a76d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search on progress ..................\n",
      "Parameters:\n",
      "{'tfidf__use_idf': (True, False)}\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "done in 14.197s\n"
     ]
    }
   ],
   "source": [
    "# Perform the grid search with pipeline and parameters\n",
    "grid_search=GridSearchCV(pipeline,parameters,n_jobs=-1,verbose=1)\n",
    "print('Grid search on progress ..................')\n",
    "print('Parameters:')\n",
    "print(parameters)\n",
    "t0=time()\n",
    "grid_search.fit(spam_collection['Message'],spam_collection['Response'])\n",
    "print('done in %0.3fs'%(time()-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7494f88",
   "metadata": {},
   "source": [
    "# NLP Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cbbfb0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9661e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_collection=pd.read_csv('C:/Users/VICKFURY/Documents/projects/Python Scripts/ml/datasets/SpamCollection', sep='\\t', names=['response','message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c5572652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     response                                            message\n",
       "0         ham  Go until jurong point, crazy.. Available only ...\n",
       "1         ham                      Ok lar... Joking wif u oni...\n",
       "2        spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3         ham  U dun say so early hor... U c already then say...\n",
       "4         ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...       ...                                                ...\n",
       "5567     spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568      ham               Will ü b going to esplanade fr home?\n",
       "5569      ham  Pity, * was in mood for that. So...any other s...\n",
       "5570      ham  The guy did some bitching but I acted like i'd...\n",
       "5571      ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2d79ed51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  response                                            message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_collection.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6bff9eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       response                 message\n",
       "count      5572                    5572\n",
       "unique        2                    5169\n",
       "top         ham  Sorry, I'll call later\n",
       "freq       4825                      30"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_collection.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5ad0a257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>response</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>4516</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         message                                                            \\\n",
       "           count unique                                                top   \n",
       "response                                                                     \n",
       "ham         4825   4516                             Sorry, I'll call later   \n",
       "spam         747    653  Please call our customer service representativ...   \n",
       "\n",
       "               \n",
       "         freq  \n",
       "response       \n",
       "ham        30  \n",
       "spam        4  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_collection.groupby('response').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9ab5a464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     response                                            message  length\n",
       "0         ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1         ham                      Ok lar... Joking wif u oni...      29\n",
       "2        spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3         ham  U dun say so early hor... U c already then say...      49\n",
       "4         ham  Nah I don't think he goes to usf, he lives aro...      61\n",
       "...       ...                                                ...     ...\n",
       "5567     spam  This is the 2nd time we have tried 2 contact u...     160\n",
       "5568      ham               Will ü b going to esplanade fr home?      36\n",
       "5569      ham  Pity, * was in mood for that. So...any other s...      57\n",
       "5570      ham  The guy did some bitching but I acted like i'd...     125\n",
       "5571      ham                         Rofl. Its true to its name      26\n",
       "\n",
       "[5572 rows x 3 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_collection['length']=spam_collection['message'].apply(len)\n",
    "spam_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ccef0e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to get rid of stopwords in messages \n",
    "def message_text_process(mess):\n",
    "#     Checking characters to see if there are punctuations\n",
    "    no_punctuation=[char for char in mess if char not in string.punctuation]\n",
    "#     forming the sentence\n",
    "    no_punctuation=\"\".join(no_punctuation)\n",
    "#     eliminating stopwords\n",
    "    return [word for word in no_punctuation.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8ef5719c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Go, jurong, point, crazy, Available, bugis, n...\n",
       "1                       [Ok, lar, Joking, wif, u, oni]\n",
       "2    [Free, entry, 2, wkly, comp, win, FA, Cup, fin...\n",
       "3        [U, dun, say, early, hor, U, c, already, say]\n",
       "4    [Nah, dont, think, goes, usf, lives, around, t...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify if function is working\n",
    "spam_collection['message'].head(5).apply(message_text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5745e1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start text processing with vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2809999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying bag of words\n",
    "bag_of_words_transformer=CountVectorizer(analyzer=message_text_process).fit(spam_collection.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7d66d4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11425\n"
     ]
    }
   ],
   "source": [
    "# Print the length of bag of words stored in vocabulary attribute\n",
    "print(len(bag_of_words_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a85abecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store bag of words for messages using transform method\n",
    "message_bag_of_words=bag_of_words_transformer.transform(spam_collection.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "67907790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tfidf and fit the bag of words into it (transformed version)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer=TfidfTransformer().fit(message_bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "48703ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 11425)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_tfidf=tfidf_transformer.transform(message_bag_of_words)\n",
    "message_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f5c82648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose Naive bayes model to detect the spam and fit the data into it\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam_model=MultinomialNB().fit(message_tfidf,spam_collection.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4b45146c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['spam'], dtype='<U4')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction\n",
    "mess1=spam_collection.message[2]\n",
    "bag_of_words_for_mess1=bag_of_words_transformer.transform([mess1])\n",
    "tfidf_mess1=tfidf_transformer.transform(bag_of_words_for_mess1)\n",
    "spam_model.predict(tfidf_mess1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ba38c83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected was spam\n"
     ]
    }
   ],
   "source": [
    "print('Expected was',spam_collection.response[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9349e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10d9036b",
   "metadata": {},
   "source": [
    "# SENTIMENTAL ANALYSIS DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "947a4271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords \n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1fc4e2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimental=pd.read_csv('C:/Users/VICKFURY/Documents/projects/Python Scripts/ml/datasets/imdb_labelled/imdb_labelled.txt',sep='\\t',names=['comment', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ae18948e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>I just got bored watching Jessice Lange take h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>Unfortunately, any virtue in this film's produ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>In a word, it is embarrassing.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>Exceptionally bad!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>All in all its an insult to one's intelligence...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>748 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               comment  labels\n",
       "0    A very, very, very slow-moving, aimless movie ...       0\n",
       "1    Not sure who was more lost - the flat characte...       0\n",
       "2    Attempting artiness with black & white and cle...       0\n",
       "3         Very little music or anything to speak of.         0\n",
       "4    The best scene in the movie was when Gerardo i...       1\n",
       "..                                                 ...     ...\n",
       "743  I just got bored watching Jessice Lange take h...       0\n",
       "744  Unfortunately, any virtue in this film's produ...       0\n",
       "745                   In a word, it is embarrassing.         0\n",
       "746                               Exceptionally bad!         0\n",
       "747  All in all its an insult to one's intelligence...       0\n",
       "\n",
       "[748 rows x 2 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "797301e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>748.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.516043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           labels\n",
       "count  748.000000\n",
       "mean     0.516043\n",
       "std      0.500077\n",
       "min      0.000000\n",
       "25%      0.000000\n",
       "50%      1.000000\n",
       "75%      1.000000\n",
       "max      1.000000"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentimental.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f581ecee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">comment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>362</td>\n",
       "      <td>361</td>\n",
       "      <td>Not recommended.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>386</td>\n",
       "      <td>384</td>\n",
       "      <td>10/10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       comment                                \n",
       "         count unique                 top freq\n",
       "labels                                        \n",
       "0          362    361  Not recommended.      2\n",
       "1          386    384             10/10      2"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentimental.groupby('labels').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8e0c12ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 748 entries, 0 to 747\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   comment  748 non-null    object\n",
      " 1   labels   748 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 11.8+ KB\n"
     ]
    }
   ],
   "source": [
    "sentimental.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c4f76016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>labels</th>\n",
       "      <th>word-length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>I just got bored watching Jessice Lange take h...</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>Unfortunately, any virtue in this film's produ...</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>In a word, it is embarrassing.</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>Exceptionally bad!</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>All in all its an insult to one's intelligence...</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>748 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               comment  labels  word-length\n",
       "0    A very, very, very slow-moving, aimless movie ...       0           87\n",
       "1    Not sure who was more lost - the flat characte...       0           99\n",
       "2    Attempting artiness with black & white and cle...       0          188\n",
       "3         Very little music or anything to speak of.         0           44\n",
       "4    The best scene in the movie was when Gerardo i...       1          108\n",
       "..                                                 ...     ...          ...\n",
       "743  I just got bored watching Jessice Lange take h...       0           63\n",
       "744  Unfortunately, any virtue in this film's produ...       0           92\n",
       "745                   In a word, it is embarrassing.         0           32\n",
       "746                               Exceptionally bad!         0           20\n",
       "747  All in all its an insult to one's intelligence...       0           75\n",
       "\n",
       "[748 rows x 3 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify length of messages and also add it as a new column\n",
    "sentimental['word-length']=sentimental.comment.apply(len)\n",
    "sentimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "912a51c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A very, very, very slow-moving, aimless movie about a distressed, drifting young man.  '"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentimental[sentimental['word-length']>50]['comment'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "601224d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text processing with countvectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer=CountVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "752a7a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get rid of stopwords present in the messages\n",
    "def message_text_process(mess):\n",
    "    cleaned_text=[char for char in mess if char not in string.punctuation]\n",
    "    cleaned_text=''.join(cleaned_text)\n",
    "    return [word for word in cleaned_text.split() if word.lower() not in stopwords.words('english')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "4389dcfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate a bag of words by applying the funtion and fit the data into it\n",
    "bag_of_words=CountVectorizer(analyzer=message_text_process).fit(sentimental.comment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "79f4cd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_bag_of_words=bag_of_words.transform(sentimental.comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5b59449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tfidf transformer and fit the bag of words into it\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer=TfidfTransformer().fit(comment_bag_of_words)\n",
    "comment_tfidf=tfidf_transformer.transform(comment_bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "089765c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(748, 3259)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "63b8a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "sentimental_model=MultinomialNB().fit(comment_tfidf,sentimental.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5148f325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted value is 1\n",
      "Expected value is 1\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "comment=sentimental.comment[4]\n",
    "bag_of_words_for_comment=bag_of_words.transform([comment])\n",
    "tfidf=tfidf_transformer.transform(bag_of_words_for_comment)\n",
    "print (\"predicted value is\",sentimental_model.predict(tfidf)[0])\n",
    "print (\"Expected value is\",sentimental.labels[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1ff54123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Topic model' is statistical modeling and used to find latent groupings in the documents based upon the words. For example, news aggregators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a8a24cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In features vector, there will be several values with zeros. The best way to save memory is to store only non zero parts of the feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b09ec21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function of the sub-module feature_extraction.text.CountVectorizer is to convert a collection of text documents to a matrix of token counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a9f639",
   "metadata": {},
   "source": [
    "# SCIKIT LEARN DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2f706f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "bbdab7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "advert_data=pd.read_csv('C:/Users/VICKFURY/Documents/projects/Python Scripts/ml/datasets/Advertising_Budget_and_Sales/Advertising Budget and Sales.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "591c437b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV Ad Budget ($)</th>\n",
       "      <th>Radio Ad Budget ($)</th>\n",
       "      <th>Newspaper Ad Budget ($)</th>\n",
       "      <th>Sales ($)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>180.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>58.4</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TV Ad Budget ($)  Radio Ad Budget ($)  Newspaper Ad Budget ($)  Sales ($)\n",
       "1             230.1                 37.8                     69.2       22.1\n",
       "2              44.5                 39.3                     45.1       10.4\n",
       "3              17.2                 45.9                     69.3        9.3\n",
       "4             151.5                 41.3                     58.5       18.5\n",
       "5             180.8                 10.8                     58.4       12.9"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advert_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f83a31c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 4)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advert_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d46841f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TV Ad Budget ($)', 'Radio Ad Budget ($)', 'Newspaper Ad Budget ($)',\n",
       "       'Sales ($)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advert_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7e8aa336",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_feature=advert_data[['TV Ad Budget ($)','Radio Ad Budget ($)','Newspaper Ad Budget ($)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "5e03a3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_feature=advert_data['Sales ($)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "fb92b952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 3)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b723b195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "e57677e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    22.1\n",
       "2    10.4\n",
       "3     9.3\n",
       "4    18.5\n",
       "5    12.9\n",
       "Name: Sales ($), dtype: float64"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "f869a521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x_feature,y_feature,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "56eef26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((160, 3), (160,), (40, 3), (40,))"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,y_train.shape,x_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "7d9c9b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression \n",
    "linreg=LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "8a402c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "f82a6bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0468431 , 0.17854434, 0.00258619])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "c890ef69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21.73577184, 16.45693776,  7.65993185, 17.89202679, 18.67730671,\n",
       "       23.86271904, 16.33623628, 13.45649226,  9.177296  , 17.36056228,\n",
       "       14.4677995 ,  9.85697601, 17.26057027, 16.71866935, 15.09530285,\n",
       "       15.58923732, 12.45188167, 17.27925151, 11.0944114 , 18.06889853,\n",
       "        9.33433055, 12.91345761,  8.7842804 , 10.46670654, 11.40303174,\n",
       "       15.03104665,  9.78479388, 19.46028647, 18.22954934, 17.1958903 ,\n",
       "       21.60304218, 14.71901407, 16.29205532, 12.36432281, 19.98831261,\n",
       "       15.37556411, 13.96678297, 10.06809496, 20.97197274,  7.45877832])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions=linreg.predict(x_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "a3fa099f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59     23.8\n",
       "41     16.6\n",
       "35      9.5\n",
       "103    14.8\n",
       "185    17.6\n",
       "199    25.5\n",
       "96     16.9\n",
       "5      12.9\n",
       "30     10.5\n",
       "169    17.1\n",
       "172    14.5\n",
       "19     11.3\n",
       "12     17.4\n",
       "90     16.7\n",
       "111    13.4\n",
       "119    15.9\n",
       "160    12.9\n",
       "36     12.8\n",
       "137     9.5\n",
       "60     18.4\n",
       "52     10.7\n",
       "17     12.5\n",
       "45      8.5\n",
       "95     11.5\n",
       "32     11.9\n",
       "163    14.9\n",
       "39     10.1\n",
       "29     18.9\n",
       "194    19.6\n",
       "28     15.9\n",
       "48     23.2\n",
       "166    11.9\n",
       "195    17.3\n",
       "178    11.7\n",
       "177    20.2\n",
       "98     15.5\n",
       "175    11.5\n",
       "74     11.0\n",
       "70     22.3\n",
       "173     7.6\n",
       "Name: Sales ($), dtype: float64"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test\n",
    "# =np.array(y_test)\n",
    "# predictions=predictions.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "b0a598c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "2d155758",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[21.73577184 16.45693776  7.65993185 17.89202679 18.67730671 23.86271904\n 16.33623628 13.45649226  9.177296   17.36056228 14.4677995   9.85697601\n 17.26057027 16.71866935 15.09530285 15.58923732 12.45188167 17.27925151\n 11.0944114  18.06889853  9.33433055 12.91345761  8.7842804  10.46670654\n 11.40303174 15.03104665  9.78479388 19.46028647 18.22954934 17.1958903\n 21.60304218 14.71901407 16.29205532 12.36432281 19.98831261 15.37556411\n 13.96678297 10.06809496 20.97197274  7.45877832].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-215-1026b815b2a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# accuracy_score(predictions,y_test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Accuracy score is not for regressionn but it is used for classification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlinreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m         \"\"\"\n\u001b[1;32m--> 238\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[0m_preprocess_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36m_decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'coo'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m         return safe_sparse_dot(X, self.coef_.T,\n\u001b[0;32m    222\u001b[0m                                dense_output=True) + self.intercept_\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    635\u001b[0m             \u001b[1;31m# If input is 1D raise error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 637\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    638\u001b[0m                     \u001b[1;34m\"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[21.73577184 16.45693776  7.65993185 17.89202679 18.67730671 23.86271904\n 16.33623628 13.45649226  9.177296   17.36056228 14.4677995   9.85697601\n 17.26057027 16.71866935 15.09530285 15.58923732 12.45188167 17.27925151\n 11.0944114  18.06889853  9.33433055 12.91345761  8.7842804  10.46670654\n 11.40303174 15.03104665  9.78479388 19.46028647 18.22954934 17.1958903\n 21.60304218 14.71901407 16.29205532 12.36432281 19.98831261 15.37556411\n 13.96678297 10.06809496 20.97197274  7.45877832].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# accuracy_score(predictions,y_test)\n",
    "# Accuracy score is not for regressionn but it is used for classification\n",
    "linreg.score(predictions,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "e608aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "dfa832f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9918855518287906"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "4c37a0b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4113417558581587"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sqrt(mean_squared_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a910b74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7411248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0e9339",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
